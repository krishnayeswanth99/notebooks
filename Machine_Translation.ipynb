{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import unicodedata,string\n",
    "import time\n",
    "import math,random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "GeForce 940MX\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/eng-fra.txt']\n"
     ]
    }
   ],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/eng-fra.txt',delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0] = data[0].str.lower()\n",
    "data[1] = data[1].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135842\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go.</td>\n",
       "      <td>va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run!</td>\n",
       "      <td>cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run!</td>\n",
       "      <td>courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wow!</td>\n",
       "      <td>ça alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fire!</td>\n",
       "      <td>au feu !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0           1\n",
       "0    go.        va !\n",
       "1   run!     cours !\n",
       "2   run!    courez !\n",
       "3   wow!  ça alors !\n",
       "4  fire!    au feu !"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.w2i = {}\n",
    "        self.i2w = {0:'UNK',1:'SOS',2:'EOS'}\n",
    "        self.n_words = 3\n",
    "        \n",
    "    def addSentence(self, sentence, add=True):\n",
    "        line = word_tokenize(sentence,language=self.name)\n",
    "        # line = sentence.split(' ')\n",
    "        # sent_tensor = torch.tensor(len(words)+1, dtype=torch.long, device=device).view(-1, 1)\n",
    "        if add:\n",
    "            for word in line:\n",
    "                self.addWord(word)\n",
    "        \n",
    "        return line\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.w2i:\n",
    "            self.w2i[word] = self.n_words\n",
    "            self.i2w[self.n_words] = word\n",
    "            self.n_words += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = Lang('english')\n",
    "fre = Lang('french')\n",
    "N_train = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5721c1598ebd477abde53272e10d5aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ded20ad9b9a4775807f4a21a87a2bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35842 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"j'en\", 'accepte', 'le', 'risque', '.'], ['i', 'accept', 'the', 'risk', '.']]\n"
     ]
    }
   ],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "pairs = [[fre.addSentence(data[1][i]),eng.addSentence(data[0][i])] for i in tqdm(range(N_train))] # if data[0][i].startswith(eng_prefixes)\n",
    "for i in tqdm(range(N_train,len(data))):\n",
    "    pairs.append([fre.addSentence(data[1][i],False),eng.addSentence(data[0][i],False)])\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromLine(lang, line):\n",
    "    return [lang.w2i[word] if word in lang.w2i else 0 for word in line]\n",
    "\n",
    "\n",
    "def tensorFromLine(lang, line):\n",
    "    indexes = indexesFromLine(lang, line)\n",
    "    indexes.append(EOS)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromLine(fre, pair[0])\n",
    "    target_tensor = tensorFromLine(eng, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS]], device=device)\n",
    "    decoder_hidden = encoder_output\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        \n",
    "        if decoder_input.item() == EOS:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_input = torch.tensor([[SOS]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for di in range(20):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "\n",
    "            if topi.item() == EOS:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(eng.i2w[topi.item()])\n",
    "            \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return ' '.join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in tqdm(range(1, n_iters + 1)):\n",
    "        \n",
    "        training_pair = training_pairs[iter - 1]\n",
    "\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(fre.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, eng.n_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617ca50a114e4ff0b52f0fb6a9932118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(encoder1, decoder1, 75000) # No of iterations should be increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandom(encoder, decoder, n=10):\n",
    "    for _ in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        input_ = pair[0]\n",
    "        target_ = pair[1]\n",
    "        input_tensor = tensorsFromPair(pair)\n",
    "        \n",
    "        ouput = evaluate(encoder, decoder, input_tensor[0])\n",
    "        \n",
    "        print('>', input_)\n",
    "        print('=', target_)\n",
    "        print('<', ouput)\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['le', 'seau', 'à', 'charbon', 'est', 'plein', '.']\n",
      "= ['the', 'coal', 'bin', 'is', 'full', '.']\n",
      "< the is is in . . . <EOS>\n",
      " \n",
      "> ['il', 'me', 'critiqua', 'pour', 'avoir', 'négligé', 'mon', 'devoir', '.']\n",
      "= ['he', 'criticized', 'me', 'for', 'neglecting', 'my', 'duty', '.']\n",
      "< he went to to my to my . . <EOS>\n",
      " \n",
      "> ['nous', 'ferions', 'mieux', 'de', 'faire', 'quelque', 'chose', '.']\n",
      "= ['we', \"'d\", 'better', 'do', 'something', '.']\n",
      "< we 'd better better better . . <EOS>\n",
      " \n",
      "> ['il', 'est', 'allé', 'faire', 'des', 'courses', '.']\n",
      "= ['he', 'went', 'shopping', '.']\n",
      "< he went to . . <EOS>\n",
      " \n",
      "> [\"c'est\", 'mon', 'tour', '.']\n",
      "= ['it', \"'s\", 'my', 'turn', '.']\n",
      "< this 's my . . <EOS>\n",
      " \n",
      "> ['aidons-le', 'afin', \"qu'il\", 'réussisse', '.']\n",
      "= ['let', \"'s\", 'help', 'him', 'so', 'that', 'he', 'will', 'succeed', '.']\n",
      "< UNK UNK UNK he he he . . <EOS>\n",
      " \n",
      "> ['mon', 'médecin', 'm', \"'\", 'a', 'conseillé', 'de', \"m'abstenir\", 'de', 'consommer', 'de', \"l'alcool\", 'pendant', 'un', 'certain', 'temps', '.']\n",
      "= ['my', 'physician', 'advised', 'me', 'to', 'refrain', 'from', 'alcohol', 'for', 'the', 'time', 'being', '.']\n",
      "< i time me time time to time . . . . . . <EOS>\n",
      " \n",
      "> ['combien', 'de', 'temps', 'faut-il', 'pour', 'aller', 'du', 'bureau', 'à', 'votre', 'maison', '?']\n",
      "= ['how', 'long', 'does', 'it', 'take', 'to', 'go', 'to', 'the', 'office', 'from', 'your', 'home', '?']\n",
      "< how long were you get to your to <EOS>\n",
      " \n",
      "> ['détends-toi', '!']\n",
      "= ['loosen', 'up', '.']\n",
      "< let 's . <EOS>\n",
      " \n",
      "> ['ils', \"m'ont\", 'accusé', \"d'être\", 'un', 'menteur', '.']\n",
      "= ['they', 'accused', 'me', 'of', 'being', 'a', 'liar', '.']\n",
      "< they would be a to . . . <EOS>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "evaluateRandom(encoder1, decoder1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
